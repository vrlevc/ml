{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0859aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "COMET_API_KEY = \"Qinr5yrH2KKQJvTcie3hVIYK3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import mitdeeplearning as mdl\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import functools\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm import tqdm\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "# assert len(tf.config.list_physical_devices('GPU')) > 0\n",
    "# assert COMET_API_KEY != \"\", \"Please insert your Comet API Key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "song = mdl.lab1.load_training_data()\n",
    "\n",
    "example_song = song[0]\n",
    "print(\"\\nExample song:\")\n",
    "print(example_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cee548",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.lab1.play_song(example_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_joined = \"\\n\\n\".join(song)\n",
    "\n",
    "vocab = sorted(set(songs_joined))\n",
    "print(f\"There are {len(vocab)} unique characters in the dataset\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eec1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa2b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_string(string):\n",
    "    vectorized_output = np.array([char2idx[char] for char in string])\n",
    "    return vectorized_output\n",
    "\n",
    "vectorized_songs = vectorize_string(songs_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('{} ---- characters mapped to int ----> {}'.format(repr(songs_joined[:10]), vectorized_songs[:10]))\n",
    "# check that vectorized_songs is a numpy array\n",
    "assert isinstance(vectorized_songs, np.ndarray), \"returned result should be a numpy array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7742237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(vectorized_songs, seq_length, batch_size):\n",
    "    n = vectorized_songs.shape[0] - 1\n",
    "    idx = np.random.choice(n-seq_length, batch_size)\n",
    "    input_batch = [vectorized_songs[i : i+seq_length] for i in idx]\n",
    "    output_batch = [vectorized_songs[i+1 : i+1+seq_length] for i in idx]\n",
    "    x_batch = np.reshape(input_batch, [batch_size, seq_length])\n",
    "    y_batch = np.reshape(output_batch, [batch_size, seq_length])\n",
    "    return x_batch, y_batch\n",
    "\n",
    "test_args = (vectorized_songs, 10, 2)\n",
    "if not mdl.lab1.test_batch_func_types(get_batch, test_args) or \\\n",
    "   not mdl.lab1.test_batch_func_shapes(get_batch, test_args) or \\\n",
    "   not mdl.lab1.test_batch_func_next_step(get_batch, test_args):\n",
    "   print(\"======\\n[FAIL] could not pass tests\")\n",
    "else:\n",
    "   print(\"======\\n[PASS] passed all tests!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc809f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\n",
    "\n",
    "for i, (input_idx, target_idx) in enumerate(zip(np.squeeze(x_batch), np.squeeze(y_batch))):\n",
    "    print(\"Step {:3d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5392a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(rnn_units):\n",
    "    return tf.keras.layers.LSTM(\n",
    "        rnn_units,\n",
    "        return_sequences=True,\n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        recurrent_activation='sigmoid',\n",
    "        stateful=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc1e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        LSTM(rnn_units),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    a = tf.constant([1, 2, 3])\n",
    "    b = tf.constant([4, 5, 6])\n",
    "    result = tf.add(a, b)\n",
    "    print(result)\n",
    "    \n",
    "model = build_model(len(vocab), embedding_dim=256, rnn_units=1024, batch_size=32)\n",
    "model.build(tf.TensorShape([32, 100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1351ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f33565",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch(vectorized_songs, seq_length=100, batch_size=32)\n",
    "pred = model(x)\n",
    "print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\n",
    "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f9a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(pred[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d3c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c0e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the loss function ###\n",
    "\n",
    "def compute_loss(labels, logits):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "    return loss\n",
    "\n",
    "example_batch_loss = compute_loss(y, pred)\n",
    "\n",
    "print(f'Prediction shape: {pred.shape} # (batch_size, sequence_length, vocab_size)')\n",
    "print(f'scalar_loss:      {example_batch_loss.numpy().mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231dfbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hypreparameter setting and optimization ###\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Model parameters:\n",
    "params = dict(\n",
    "    num_training_iterations = 300, # Increase this to train longer\n",
    "    batch_size = 8, # Experiment between 1 and 64\n",
    "    seq_length = 100, # Experiment between 50 and 500\n",
    "    learning_rate = 5e-3, # Experiment between 1e-5 and 1e-1\n",
    "    embeding_dim = 256,\n",
    "    rnn_units = 1024, # Experiment berween 1 and 2048\n",
    ")\n",
    "\n",
    "# Checkpoint location:\n",
    "chechpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(chechpoint_dir, \"my_ckpt.weights.h5\")\n",
    "os.makedirs(checkpoint_prefix, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a Comet expriment to track our training run ###\n",
    "\n",
    "def create_experiment():\n",
    "    # end any prior experiments\n",
    "    if 'experiment' in locals():\n",
    "        experiment.end()\n",
    "        \n",
    "    # initiate the comet experiment for tracking\n",
    "    experiment = comet_ml.Experiment(\n",
    "        api_key=COMET_API_KEY,\n",
    "        project_name=\"6S191_Lab1_Part2\"\n",
    "    )\n",
    "    # log our hyperparameters, defined above, to the experiment\n",
    "    for param, value in params.items():\n",
    "        experiment.log_parameter(param, value)\n",
    "    experiment.flush()\n",
    "    \n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7559445",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define optimizer and training operation ###\n",
    "\n",
    "model = build_model(vocab_size, params[\"embeding_dim\"], params[\"rnn_units\"], params[\"batch_size\"])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(params[\"learning_rate\"])\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    # Use tf.GradientTape()\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        '''feed the current input into the model and generate predictions'''\n",
    "        y_hat = model(x) \n",
    "\n",
    "        '''compute the loss!'''\n",
    "        loss = compute_loss(y, y_hat) \n",
    "\n",
    "        # Now, compute the gradients\n",
    "        grads = tape.gradient(loss, model.trainable_variables) \n",
    "\n",
    "    # Apply the gradients to the optimizer so it can update the model accordingly\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "##################\n",
    "# Begin training!#\n",
    "##################\n",
    "\n",
    "history = []\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
    "experiment = create_experiment()\n",
    "\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "for iter in tqdm(range(params[\"num_training_iterations\"])):\n",
    "\n",
    "  # Grab a batch and propagate it through the network\n",
    "  x_batch, y_batch = get_batch(vectorized_songs, params[\"seq_length\"], params[\"batch_size\"])\n",
    "  loss = train_step(x_batch, y_batch)\n",
    "\n",
    "  # log the loss to the Comet interface! we will be able to track it there.\n",
    "  experiment.log_metric(\"loss\", loss.numpy().mean(), step=iter)\n",
    "  # Update the progress bar and also visualize within notebook\n",
    "  history.append(loss.numpy().mean())\n",
    "  plotter.plot(history)\n",
    "\n",
    "  # Update the model with the changed weights!\n",
    "  if iter % 100 == 0:\n",
    "    model.save_weights(checkpoint_prefix)\n",
    "\n",
    "# Save the trained model and the weights\n",
    "model.save_weights(checkpoint_prefix)\n",
    "experiment.flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
