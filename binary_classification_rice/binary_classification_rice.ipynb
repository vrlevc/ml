{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load the imports\n",
    "\n",
    "import io\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# The following lines adjust the granularity of reporting.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "print(\"Ran the import statements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load the dataset\n",
    "rice_dataset_raw = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/Rice_Cammeo_Osmancik.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "# Read and provide statistics on the dataset.\n",
    "rice_dataset = rice_dataset_raw[[\n",
    "    'Area',\n",
    "    'Perimeter',\n",
    "    'Major_Axis_Length',\n",
    "    'Minor_Axis_Length',\n",
    "    'Eccentricity',\n",
    "    'Convex_Area',\n",
    "    'Extent',\n",
    "    'Class',\n",
    "]]\n",
    "\n",
    "rice_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary statistics above, answer the following questions:\n",
    "- What are the min and max lengths (major axis length, given in pixels) of the rice grains?\n",
    "- What is the range of areas between the smallest and largest rice grains?\n",
    "- How many standard deviations (std) is the largest rice grain's perimeter from the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Solutions (run the cell to get the answers)\n",
    "\n",
    "print(\n",
    "    f'The shortest grain is {rice_dataset.Major_Axis_Length.min():.1f}px long,'\n",
    "    f' while the longest is {rice_dataset.Major_Axis_Length.max():.1f}px.'\n",
    ")\n",
    "print(\n",
    "    f'The smallest rice grain has an area of {rice_dataset.Area.min()}px, while'\n",
    "    f' the largest has an area of {rice_dataset.Area.max()}px.'\n",
    ")\n",
    "print(\n",
    "    'The largest rice grain, with a perimeter of'\n",
    "    f' {rice_dataset.Perimeter.max():.1f}px, is'\n",
    "    f' ~{(rice_dataset.Perimeter.max() - rice_dataset.Perimeter.mean())/rice_dataset.Perimeter.std():.1f} standard'\n",
    "    f' deviations ({rice_dataset.Perimeter.std():.1f}) from the mean'\n",
    "    f' ({rice_dataset.Perimeter.mean():.1f}px).'\n",
    ")\n",
    "print(\n",
    "    f'This is calculated as: ({rice_dataset.Perimeter.max():.1f} -'\n",
    "    f' {rice_dataset.Perimeter.mean():.1f})/{rice_dataset.Perimeter.std():.1f} ='\n",
    "    f' {(rice_dataset.Perimeter.max() - rice_dataset.Perimeter.mean())/rice_dataset.Perimeter.std():.1f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create five 2D plots of the features against each other, color-coded by class.\n",
    "n=0\n",
    "for x_axis_data, y_axis_data in [\n",
    "    ('Area', 'Eccentricity'),\n",
    "    ('Convex_Area', 'Perimeter'),\n",
    "    ('Major_Axis_Length', 'Minor_Axis_Length'),\n",
    "    ('Perimeter', 'Extent'),\n",
    "    ('Eccentricity', 'Major_Axis_Length'),\n",
    "]:\n",
    "  px.scatter(rice_dataset, x=x_axis_data, y=y_axis_data, color='Class').write_html(f'plot{n}.html')\n",
    "  n=n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title One possible solution\n",
    "\n",
    "# Plot major and minor axis length and eccentricity, with observations\n",
    "# color-coded by class.\n",
    "px.scatter_3d(\n",
    "    rice_dataset,\n",
    "    x='Eccentricity',\n",
    "    y='Area',\n",
    "    z='Major_Axis_Length',\n",
    "    color='Class',\n",
    ").write_html('plot_3D.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Z-scores of each numerical column in the raw data and write\n",
    "# them into a new DataFrame named df_norm.\n",
    "\n",
    "feature_mean = rice_dataset.mean(numeric_only=True)\n",
    "feature_std  = rice_dataset.std(numeric_only=True)\n",
    "numerical_features = rice_dataset.select_dtypes('number').columns\n",
    "normalized_dataset = (rice_dataset[numerical_features] - feature_mean) / feature_std\n",
    "\n",
    "# Copy the class to the new dataframe\n",
    "normalized_dataset['Class'] = rice_dataset['Class']\n",
    "\n",
    "# Examine some of the values of the normalized training set. Notice that most\n",
    "# Z-scores fall between -2 and +2.\n",
    "normalized_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seeds:\n",
    "#\n",
    "# To make experiments reproducible, we set the seed of the random number generators. \n",
    "# This means that the order in which the data is shuffled, the values of the random \n",
    "# weight initializations, etc, will all be the same each time the colab is run.\n",
    "\n",
    "keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column setting the Cammeo label to '1' and the Osmancik label to '0'\n",
    "# then show 10 randomly selected rows.\n",
    "normalized_dataset['Class_Bool'] = (\n",
    "    # Returns true if class is Cammeo, and false if class is Osmancik\n",
    "    normalized_dataset['Class'] == 'Cammeo'\n",
    ").astype(int)\n",
    "\n",
    "normalized_dataset.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indices at the 80th and 90th percentiles\n",
    "number_semples = len(normalized_dataset)\n",
    "index_80th = round(number_semples * 0.8)\n",
    "index_90th = index_80th + round(number_semples * 0.1)\n",
    "\n",
    "# Randomize order and split into train, validation, and test with a .8, .1, .1 split\n",
    "shufled_dataset = normalized_dataset.sample(frac=1, random_state=100)\n",
    "train_data = shufled_dataset.iloc[0:index_80th]\n",
    "validation_data = shufled_dataset.iloc[index_80th:index_90th]\n",
    "test_data = shufled_dataset.iloc[index_90th:]\n",
    "\n",
    "# Show the first five rows of the last split\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['Class', 'Class_Bool']\n",
    "\n",
    "train_features = train_data.drop(columns=label_columns)\n",
    "train_labels = train_data['Class_Bool'].to_numpy()\n",
    "validation_features = validation_data.drop(columns=label_columns)\n",
    "validation_labels = validation_data['Class_Bool'].to_numpy()\n",
    "test_features = test_data.drop(columns=label_columns)\n",
    "test_labels = test_data['Class_Bool'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the features we'll train our model on.\n",
    "input_features = [\n",
    "    'Eccentricity',\n",
    "    'Major_Axis_Length',\n",
    "    'Area',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define the functions that create and train a model.\n",
    "\n",
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass()\n",
    "class ExperimentSettings:\n",
    "    \"\"\"List the hyperparameters and input features used to train a model\"\"\"\n",
    "    \n",
    "    learning_rate : float\n",
    "    number_epochs : int\n",
    "    batch_size : int\n",
    "    classification_threshold : float\n",
    "    input_features : list[str]\n",
    "    \n",
    "@dataclasses.dataclass()\n",
    "class Experiment:\n",
    "    \"\"\"Stores the settings used for a training run and the resulting model.\"\"\"\n",
    "  \n",
    "    name: str\n",
    "    settings: ExperimentSettings\n",
    "    model: keras.Model\n",
    "    epochs: np.ndarray\n",
    "    metrics_history: keras.callbacks.History\n",
    "\n",
    "    def get_final_metric_value(self, metric_name: str) -> float:\n",
    "        \"\"\"Gets the final value of the given metric for this experiment.\"\"\"\n",
    "        if metric_name not in self.metrics_history:\n",
    "            raise ValueError(\n",
    "                f'Unknown metric {metric_name}: available metrics are'\n",
    "                f' {list(self.metrics_history.columns)}'\n",
    "            )\n",
    "        return self.metrics_history[metric_name].iloc[-1]\n",
    "\n",
    "def create_model(\n",
    "    settings: ExperimentSettings,\n",
    "    metrics: list[keras.metrics.Metric],\n",
    ") -> keras.Model:\n",
    "    \"\"\"Create and compile a simple classification model.\"\"\"\n",
    "    model_inputs = [\n",
    "        keras.Input(name=feature, shape=(1,))\n",
    "        for feature in settings.input_features\n",
    "    ]\n",
    "    # Use a Concatenate layer to assemble the different inputs into a single\n",
    "    # tensor which will be given as input to the Dense layer.\n",
    "    # For example: [input_1[0][0], input_2[0][0]]\n",
    "    concatenated_inputs = keras.layers.Concatenate()(model_inputs)\n",
    "    dense = keras.layers.Dense(\n",
    "        units=1, input_shape=(1,), name='dense_layer', activation=keras.activations.sigmoid\n",
    "    )\n",
    "    model_output = dense(concatenated_inputs)\n",
    "    model = keras.Model(inputs=model_inputs, outputs=model_output)\n",
    "    # Call the compile method to transform the layers into a model that\n",
    "    # Keras can execute.  Notice that we're using a different loss\n",
    "    # function for classification than for regression.\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.RMSprop(\n",
    "            settings.learning_rate\n",
    "        ),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "def train_model(\n",
    "    experiment_name: str,\n",
    "    model: keras.Model,\n",
    "    dataset: pd.DataFrame,\n",
    "    labels: np.ndarray,\n",
    "    settings: ExperimentSettings,\n",
    ") -> Experiment:\n",
    "    \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "    # The x parameter of keras.Model.fit can be a list of arrays, where\n",
    "    # each array contains the data for one feature.\n",
    "    features = {\n",
    "        feature_name: np.array(dataset[feature_name])\n",
    "        for feature_name in settings.input_features\n",
    "    }\n",
    "    \n",
    "    history = model.fit(\n",
    "        x=features,\n",
    "        y=labels,\n",
    "        batch_size=settings.batch_size,\n",
    "        epochs=settings.number_epochs,\n",
    "    )\n",
    "    \n",
    "    return Experiment(\n",
    "        name=experiment_name,\n",
    "        settings=settings,\n",
    "        model=model,\n",
    "        epochs=history.epoch,\n",
    "        metrics_history=pd.DataFrame(history.history),\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define the plotting function.\n",
    "def plot_experiment_metrics(experiment: Experiment, metrics: list[str]):\n",
    "    \"\"\"Plot a curve of one or more metrics for different epochs.\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for metric in metrics:\n",
    "        plt.plot(\n",
    "            experiment.epochs, experiment.metrics_history[metric], label=metric\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Metric value\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    \n",
    "print(\"Defined the plot_curve function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our first experiment settings.\n",
    "settings = ExperimentSettings(\n",
    "    learning_rate=0.001,\n",
    "    number_epochs=60,\n",
    "    batch_size=100,\n",
    "    classification_threshold=0.35,\n",
    "    input_features=input_features,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy', threshold=settings.classification_threshold\n",
    "    ),\n",
    "    keras.metrics.Precision(\n",
    "        name='precision', thresholds=settings.classification_threshold\n",
    "    ),\n",
    "    keras.metrics.Recall(\n",
    "        name='recall', thresholds=settings.classification_threshold\n",
    "    ),\n",
    "    keras.metrics.AUC(num_thresholds=100, name='auc'),\n",
    "]\n",
    "\n",
    "# Establish the model's topography.\n",
    "model = create_model(settings, metrics)\n",
    "\n",
    "# Train the model on the training set.\n",
    "experiment = train_model(\n",
    "    'baseline', model, train_features, train_labels, settings\n",
    ")\n",
    "\n",
    "# Plot metrics vs. epochs\n",
    "plot_experiment_metrics(experiment, ['accuracy', 'precision', 'recall'])\n",
    "plot_experiment_metrics(experiment, ['auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_experiment(\n",
    "    experiment: Experiment, test_dataset: pd.DataFrame, test_labels: np.array\n",
    ") -> dict[str, float]:\n",
    "  features = {\n",
    "      feature_name: np.array(test_dataset[feature_name])\n",
    "      for feature_name in experiment.settings.input_features\n",
    "  }\n",
    "  return experiment.model.evaluate(\n",
    "      x=features,\n",
    "      y=test_labels,\n",
    "      batch_size=settings.batch_size,\n",
    "      verbose=0, # Hide progress bar\n",
    "      return_dict=True,\n",
    "  )\n",
    "\n",
    "\n",
    "def compare_train_test(experiment: Experiment, test_metrics: dict[str, float]):\n",
    "  print('Comparing metrics between train and test:')\n",
    "  for metric, test_value in test_metrics.items():\n",
    "    print('------')\n",
    "    print(f'Train {metric}: {experiment.get_final_metric_value(metric):.4f}')\n",
    "    print(f'Test {metric}:  {test_value:.4f}')\n",
    "\n",
    "\n",
    "# Evaluate test metrics\n",
    "test_metrics = evaluate_experiment(experiment, test_features, test_labels)\n",
    "compare_train_test(experiment, test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution\n",
    "# Features used to train the model on.\n",
    "# Specify all features.\n",
    "all_input_features = [\n",
    "  'Eccentricity',\n",
    "  'Major_Axis_Length',\n",
    "  'Minor_Axis_Length',\n",
    "  'Area',\n",
    "  'Convex_Area',\n",
    "  'Perimeter',\n",
    "  'Extent',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_all_features = ExperimentSettings(\n",
    "    learning_rate=0.001,\n",
    "    number_epochs=60,\n",
    "    batch_size=100,\n",
    "    classification_threshold=0.5,\n",
    "    input_features=all_input_features,\n",
    ")\n",
    "\n",
    "# Modify the following definition of METRICS to generate\n",
    "# not only accuracy and precision, but also recall:\n",
    "metrics = [\n",
    "    keras.metrics.BinaryAccuracy(\n",
    "        name='accuracy',\n",
    "        threshold=settings_all_features.classification_threshold,\n",
    "    ),\n",
    "    keras.metrics.Precision(\n",
    "        name='precision',\n",
    "        thresholds=settings_all_features.classification_threshold,\n",
    "    ),\n",
    "    keras.metrics.Recall(\n",
    "        name='recall', thresholds=settings_all_features.classification_threshold\n",
    "    ),\n",
    "    keras.metrics.AUC(num_thresholds=100, name='auc'),\n",
    "]\n",
    "\n",
    "# Establish the model's topography.\n",
    "model_all_features = create_model(settings_all_features, metrics)\n",
    "\n",
    "# Train the model on the training set.\n",
    "experiment_all_features = train_model(\n",
    "    'all features',\n",
    "    model_all_features,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    settings_all_features,\n",
    ")\n",
    "\n",
    "# Plot metrics vs. epochs\n",
    "plot_experiment_metrics(\n",
    "    experiment_all_features, ['accuracy', 'precision', 'recall']\n",
    ")\n",
    "plot_experiment_metrics(experiment_all_features, ['auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics_all_features = evaluate_experiment(\n",
    "    experiment_all_features, test_features, test_labels\n",
    ")\n",
    "compare_train_test(experiment_all_features, test_metrics_all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define function to compare experiments\n",
    "\n",
    "def compare_experiment(experiments: list[Experiment],\n",
    "                       metrics_of_interest: list[str],\n",
    "                       test_dataset: pd.DataFrame,\n",
    "                       test_labels: np.array):\n",
    "  # Make sure that we have all the data we need.\n",
    "  for metric in metrics_of_interest:\n",
    "    for experiment in experiments:\n",
    "      if metric not in experiment.metrics_history:\n",
    "        raise ValueError(f'Metric {metric} not available for experiment {experiment.name}')\n",
    "\n",
    "  fig = plt.figure(figsize=(12, 12))\n",
    "  ax = fig.add_subplot(2, 1, 1)\n",
    "\n",
    "  colors = [f'C{i}' for i in range(len(experiments))]\n",
    "  markers = ['.', '*', 'd', 's', 'p', 'x']\n",
    "  marker_size = 10\n",
    "\n",
    "  ax.set_title('Train metrics')\n",
    "  for i, metric in enumerate(metrics_of_interest):\n",
    "    for j, experiment in enumerate(experiments):\n",
    "      plt.plot(experiment.epochs, experiment.metrics_history[metric], markevery=4,\n",
    "               marker=markers[i], markersize=marker_size, color=colors[j])\n",
    "\n",
    "  # Add custom legend to show what the colors and markers mean\n",
    "  legend_handles = []\n",
    "  for i, metric in enumerate(metrics_of_interest):\n",
    "    legend_handles.append(Line2D([0], [0], label=metric, marker=markers[i],\n",
    "                                 markersize=marker_size, c='k'))\n",
    "  for i, experiment in enumerate(experiments):\n",
    "    legend_handles.append(Line2D([0], [0], label=experiment.name, color=colors[i]))\n",
    "\n",
    "  ax.set_xlabel(\"Epoch\")\n",
    "  ax.set_ylabel(\"Metric value\")\n",
    "  ax.grid()\n",
    "  ax.legend(handles=legend_handles)\n",
    "\n",
    "  ax = fig.add_subplot(2, 1, 2)\n",
    "  spacing = 0.3\n",
    "  n_bars = len(experiments)\n",
    "  bar_width = (1 - spacing)/n_bars\n",
    "  for i, experiment in enumerate(experiments):\n",
    "    test_metrics = evaluate_experiment(experiment, test_dataset, test_labels)\n",
    "    x = np.arange(len(metrics_of_interest)) + bar_width * (i + 1/2 - n_bars/2)\n",
    "    ax.bar(x, [test_metrics[metric] for metric in metrics_of_interest], width=bar_width, label=experiment.name)\n",
    "  ax.set_xticks(np.arange(len(metrics_of_interest)), metrics_of_interest)\n",
    "\n",
    "  ax.set_title('Test metrics')\n",
    "  ax.set_ylabel('Metric value')\n",
    "  ax.set_axisbelow(True) # Put the grid behind the bars\n",
    "  ax.grid()\n",
    "  ax.legend()\n",
    "\n",
    "print('Defined function to compare experiments.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_experiment([experiment, experiment_all_features],\n",
    "                   ['accuracy', 'auc'],\n",
    "                   test_features, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
